apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: llm-benchmark
  namespace: argocd
spec:
  project: default
  source:
    repoURL: https://bjw-s-labs.github.io/helm-charts
    chart: app-template
    targetRevision: 3.2.1
    helm:
      values: |
        controllers:
          main:
            pod:
              runtimeClassName: nvidia
            containers:
              main:
                image:
                  repository: vllm/vllm-openai
                  tag: latest
                env:
                  OLLAMA_BASE_URL: "http://ollama.ollama.svc.cluster.local:11434"
                resources:
                  requests:
                    cpu: 500m
                    memory: 1Gi
                    nvidia.com/gpu: 1
                  limits:
                    cpu: 1000m
                    memory: 4Gi
                    nvidia.com/gpu: 1
        service:
          main:
            controller: main
            ports:
              http:
                port: 8000
        ingress:
          main:
            enabled: true
            annotations:
              kubernetes.io/ingress.class: "nginx"
              external-dns.alpha.kubernetes.io/hostname: "benchmark.unbund.com"
              external-dns.alpha.kubernetes.io/target: "22051e43-bc84-438d-b7c3-b829c1e2f6ad.cfargotunnel.com"
              external-dns.alpha.kubernetes.io/cloudflare-proxied: "true"
            hosts:
              - host: benchmark.unbund.com
                paths:
                  - path: /
                    pathType: Prefix
                    service:
                      identifier: main
                      port: http
  destination:
    server: https://kubernetes.default.svc
    namespace: llm-benchmark
  syncPolicy:
    automated: { prune: true, selfHeal: true }
    syncOptions: [CreateNamespace=true]
